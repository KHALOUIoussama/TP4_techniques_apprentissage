{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT603 - Devoir 4\n",
    "\n",
    "* Classifieur linéaire avec un réseau de neurones à une couche\n",
    "* Fonction de perte: entropie-croisée\n",
    "* Descente de gradient\n",
    "* Recherche d'hyperparamètres\n",
    "* Visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Imporation des bibliothèques python générales\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "'''\n",
    " Imporation des bibliothèques spécifiques au devoir\n",
    "'''\n",
    "import utils\n",
    "from linear_classifier import LinearClassifier\n",
    "from two_layer_classifier import TwoLayerClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ère partie du devoir : la classification linéaire par régression logistique\n",
    "\n",
    "**Préparation des données**\n",
    "\n",
    "Nous utilisons une fonction bien connue de sklearn pour générer un jeu de données nommée [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).  Cette base de données comprend 1000 éléments distribués dans 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des données\n",
    "X_, y_ = make_classification(1000,n_features=2, n_redundant=0, n_informative=2,n_clusters_per_class=1, n_classes=3,random_state=6)\n",
    "\n",
    "# Centrer et réduire les données (moyenne = 0, écart-type = 1)\n",
    "mean = np.mean(X_, axis=0)\n",
    "std = np.std(X_, axis=0)\n",
    "X_ = (X_ - mean) / std\n",
    "\n",
    "# Afficher\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_[:, 0], X_[:, 1], c=y_, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparons le jeu de données en trois parties: l'ensemble **d'entraînement**, de **validation** et de **test** (_train_, _val_ et _test_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_val = 200\n",
    "num_test = 200\n",
    "num_train = 600\n",
    "np.random.seed(1)\n",
    "idx = np.random.permutation(len(X_))\n",
    "\n",
    "train_idx = idx[:num_train]\n",
    "val_idx = idx[num_train:num_train + num_val]\n",
    "test_idx = idx[-num_test:]\n",
    "\n",
    "X_train = X_[train_idx]\n",
    "y_train = y_[train_idx]\n",
    "X_val = X_[val_idx]\n",
    "y_val = y_[val_idx]\n",
    "X_test = X_[test_idx]\n",
    "y_test = y_[test_idx]\n",
    "\n",
    "# Afficher\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.title('Data train')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.title('Data Validation')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.title('Data test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions que les données sont valides en entraînant un SVM de la librairie `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = utils.test_sklearn_svm(X_train, y_train, X_test, y_test)\n",
    "print('Test accuracy: {:.3f}'.format(accu))\n",
    "if accu < 0.7:\n",
    "    print('ERREUR: L\\'accuracy est trop faible. Il y a un problème avec les données. Vous pouvez essayer de refaire le mélange (case ci-haut).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémenter un classifieur logistique linéaire\n",
    "\n",
    "Dans `linear_classifer.py`, implémenter la `cross_entropy_loss` (entropie croisée) ainsi que les méthodes avec l'indicatif TODO\n",
    "\n",
    "Une fois fait, vérifier votre implémentation avec les cases qui suivent.\n",
    "\n",
    "Commençons par quelques **Sanity Checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En premier, vérifier la prédiction du modèle, la \"forward pass\"\n",
    "# 1. Générer le modèle avec des poids W aléatoires\n",
    "model = LinearClassifier(X_train, y_train, X_val, y_val, num_classes=3, bias=True)\n",
    "\n",
    "# 2. Appeler la fonction qui calcule l'accuracy et la loss moyenne pour l'ensemble des données d'entraînement\n",
    "_, loss = model.global_accuracy_and_cross_entropy_loss(X_train,y_train)\n",
    "\n",
    "# 3. Comparer au résultat attendu\n",
    "loss_attendu = -np.log(1.0/3.0) # résultat aléatoire attendu soit -log(1/nb_classes)\n",
    "print('Sortie: {}  Attendu: {}'.format(loss, loss_attendu))\n",
    "if abs(loss - loss_attendu) > 0.05:\n",
    "    print('ERREUR: la sortie de la fonction est incorrecte.')\n",
    "else:\n",
    "    print('SUCCÈS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification: Vous devez pouvoir faire du surapprentissage sur quelques échantillons.\n",
    "# Si l'accuracy reste faible, votre implémentation a un bogue.\n",
    "n_check = 5\n",
    "X_check = X_train[:n_check]\n",
    "y_check = y_train[:n_check]\n",
    "model = LinearClassifier(X_check, y_check, X_val, y_val, num_classes=3, bias=True)\n",
    "loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(num_epochs=10, lr=1.0, l2_reg=0.0)\n",
    "accu_train_finale = accu_train_curve[-1]\n",
    "print('Accuracy d\\'entraînement, devrait être 1.0: {:.3f}'.format(accu_train_finale))\n",
    "if accu_train_finale < 0.9999:\n",
    "    print('ATTENTION: L\\'accuracy n\\'est pas 100%.')\n",
    "    utils.plot_curves(loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve)\n",
    "else:\n",
    "    print('SUCCÈS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lorsque ça fonctionne, maintenant testons l'effet du terme de régularisation l2_reg.  **Augmenter le terme l2_reg devrait augmenter la loss et, à la limite, faire décroire l'accuracy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prenons encore un petit échantillon et testons différentes valeurs de l2_reg\n",
    "n_check = 5\n",
    "X_check = X_train[:n_check]\n",
    "y_check = y_train[:n_check]\n",
    "model = LinearClassifier(X_check, y_check, X_val, y_val, num_classes=3, bias=True)\n",
    "\n",
    "for l2_r in np.arange(0,1,0.05):\n",
    "    loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(num_epochs=10, lr=1.0, l2_reg=l2_r)\n",
    "    print('l2_reg= {:.4f} >> Loss/accuracy d\\'entraînement : {:.3f} {:.3f}'.format(l2_r,loss_train_curve[-1],accu_train_curve[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vous pouvez maintenant essayer d'entraîner le modèle avec les données complètes\n",
    "\n",
    "Normalement la loss devrait décroitre et l'accuracy augmenter en fonction des epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie et entraîne notre modèle; cette fois-ci avec les données complètes.\n",
    "model = LinearClassifier(X_train, y_train, X_val, y_val, num_classes=3, bias=True)\n",
    "loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(lr=0.001,num_epochs=25, l2_reg=0.01)\n",
    "\n",
    "# Illustration de la loss et de l'accuracy (le % de biens classés) à chaque itération     \n",
    "utils.plot_curves(loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve)\n",
    "\n",
    "print('[Training]   Loss: {:.3f}   Accuracy: {:.3f}'.format(loss_train_curve[-1], accu_train_curve[-1]))\n",
    "print('[Validation] Loss: {:.3f}   Accuracy: {:.3f}'.format(loss_val_curve[-1], accu_val_curve[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rechercher de meilleurs hyperparamètres\n",
    "\n",
    "Nous allons effectuer une recherche sur ces hyperparamètres:\n",
    "\n",
    "* `learning rate`: La longueur des pas lors de la descente de gradient\n",
    "* `L2 regularization`: La pénalité sur la taille des poids dans `W`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_choices = [1e-2, 1e-1, 1.0, 10.0]\n",
    "reg_choices = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "lr_decay = 0.995  # learning rate is multiplied by this factor after each step\n",
    "\n",
    "best_accu = -1\n",
    "best_params = None\n",
    "best_model = None\n",
    "best_curves = None\n",
    "\n",
    "for lr, reg in itertools.product(lr_choices, reg_choices):\n",
    "    params = (lr, reg)\n",
    "    curves = model.train(num_epochs=25, lr=lr, l2_reg=reg, lr_decay=lr_decay)\n",
    "    loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = curves\n",
    "    \n",
    "    val_accu = accu_val_curve[-1]\n",
    "    if val_accu > best_accu:\n",
    "        print('Best val accuracy: {:.3f} | lr: {:.0e} | l2_reg: {:.0e}'.format(val_accu, lr, reg))\n",
    "        best_accu = val_accu\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "        best_curves = curves\n",
    "        \n",
    "model = best_model\n",
    "utils.plot_curves(*best_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vérifier la généralisation sur l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On ré-entraîne le modèle avec les meilleurs hyper-paramètres\n",
    "lr, reg = best_params\n",
    "model.train(num_epochs=25, lr=lr, l2_reg=reg, lr_decay=lr_decay)\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "accu = (pred == y_test).mean()\n",
    "print('Test accuracy: {:.3f}'.format(accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracer les frontières de décision\n",
    "\n",
    "Nous allons créer une grille de points 2D qui recouvre les données, et nous allons prédire la classe pour chacun de ces points dans l'espace. Cela nous permettra de visualiser les frontières de décision apprises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01  # contrôle la résolution de la grille\n",
    "x_min, x_max = X_[:, 0].min() - .5, X_[:, 0].max() + .5  # Limites de la grille\n",
    "y_min, y_max = X_[:, 1].min() - .5, X_[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))  # Créer la grille\n",
    "\n",
    "X_predict = np.c_[xx.ravel(), yy.ravel()]  # Convertir la grille en une liste de points\n",
    "Z = model.predict(X_predict)  # Classifier chaque point de la grille\n",
    "Z = Z.reshape(xx.shape)  # Remettre en 2D\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)  # Colorier les cases selon les prédictions\n",
    "\n",
    "X_plot, y_plot = X_train, y_train\n",
    "X_plot, y_plot = X_train, y_train\n",
    "plt.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolors='k', cmap=plt.cm.Paired)  # Tracer les données\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.title('Frontières de décision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2e partie du devoir: la classification non-linéaire\n",
    "\n",
    "Nous essaierons maintenant d'apprendre des frontières de décision **non-linéaires**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choisissez le type de données que vous voulez \n",
    "\n",
    "# NOTE IMPORTANTE: on vous encourage à tester différentes bases de données.  Ceci dit, \n",
    "# votre solution sera testée avec Ncircles (N=4).  Vous devez donc tester cette option.\n",
    "dataset_type = 'Ncircles'\n",
    "if dataset_type == 'moons':\n",
    "    X_, y_ = sklearn.datasets.make_moons(n_samples=200, noise=0.5)\n",
    "    num_classes = 2\n",
    "elif dataset_type == 'gaussian_quantiles':\n",
    "    X_, y_ = sklearn.datasets.make_gaussian_quantiles(n_samples=200, n_classes=2)\n",
    "    num_classes = 2\n",
    "elif dataset_type == '4blobs':\n",
    "    d = 4\n",
    "    c1a = np.random.randn(50, 2)\n",
    "    c1b = np.random.randn(50, 2) + (d, d)\n",
    "    c2a = np.random.randn(50, 2) + (0, d)\n",
    "    c2b = np.random.randn(50, 2) + (d, 0)\n",
    "    X_ = np.concatenate([c1a, c1b, c2a, c2b], axis=0)\n",
    "    y_ = np.array([0] * 100 + [1] * 100)\n",
    "    num_classes = 2\n",
    "elif dataset_type == '2circles':\n",
    "    X_, y_ = sklearn.datasets.make_circles(n_samples=200)\n",
    "    num_classes = 2\n",
    "elif dataset_type == 'Ncircles':\n",
    "    samples_per_class = 100\n",
    "    num_classes = 4\n",
    "    angles = np.linspace(0, 2*np.pi, samples_per_class)\n",
    "    radius = 1.0 + np.arange(num_classes) * 0.3\n",
    "    px = np.cos(angles[:, None]) * radius[None, :]  # (100, 3)\n",
    "    py = np.sin(angles[:, None]) * radius[None, :]  # (100, 3)\n",
    "    X_ = np.stack([px, py], axis=-1).reshape((samples_per_class * num_classes, 2))\n",
    "    X_ += np.random.randn(len(X_[:, 0]),2)/8\n",
    "    y_ = np.array(list(range(num_classes)) * samples_per_class)\n",
    "else:\n",
    "    print('Invalid dataset type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_[:, 0], X_[:, 1], c=y_, cmap=plt.cm.Paired)\n",
    "plt.title('Données complètes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proportion = 0.5\n",
    "val_proportion = 0.2\n",
    "num_train = int(len(X_) * train_proportion)\n",
    "num_val = int(len(X_) * val_proportion)\n",
    "\n",
    "np.random.seed(0)\n",
    "idx = np.random.permutation(len(X_))\n",
    "\n",
    "train_idx = idx[:num_train]\n",
    "val_idx = idx[num_train:num_train + num_val]\n",
    "test_idx = idx[num_train + num_val:]\n",
    "\n",
    "X_train = X_[train_idx]\n",
    "y_train = y_[train_idx]\n",
    "X_val = X_[val_idx]\n",
    "y_val = y_[val_idx]\n",
    "X_test = X_[test_idx]\n",
    "y_test = y_[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichons maintenant les données d'entraînement, de validation et de test.\n",
    "plt.figure()\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired)\n",
    "plt.title('Train')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=plt.cm.Paired)\n",
    "plt.title('Validation')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Paired)\n",
    "plt.title('Test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entraîner avec la descente de gradient\n",
    "\n",
    "Dans `two_layer_classifyer.py`, complétez les méthodes avec l'indicatif `TODO`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons avec quelques **Sanity checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_neurons = 10\n",
    "model = TwoLayerClassifier(X_train, y_train, X_val, y_val,\n",
    "                           num_features=2, num_hidden_neurons=num_hidden_neurons, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que la sortie du réseau initialisé au hasard donne bien une prédiction égale pour chaque classe\n",
    "num_hidden_neurons = 10\n",
    "model = TwoLayerClassifier(X_train, y_train, X_val, y_val,\n",
    "                           num_features=2, num_hidden_neurons=num_hidden_neurons, num_classes=num_classes)\n",
    "\n",
    "# 2. Appeler la fonction qui calcule l'accuracy et la loss moyenne pour l'ensemble des données d'entraînement\n",
    "_, loss = model.global_accuracy_and_cross_entropy_loss(X_train,y_train,0)\n",
    "\n",
    "# 3. Comparer au résultat attendu\n",
    "loss_attendu = -np.log(1.0/num_classes) # résultat aléatoire attendu soit -log(1/nb_classes)\n",
    "print('Sortie: {}  Attendu: {}'.format(loss, loss_attendu))\n",
    "if abs(loss - loss_attendu) > 0.05:\n",
    "    print('ERREUR: la sortie de la fonction est incorrecte.')\n",
    "else:\n",
    "    print('SUCCÈS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que le fait d'augmenter la régularisation L2 augmente également la loss\n",
    "for l2_r in np.arange(0,2,0.1):\n",
    "    _, loss = model.global_accuracy_and_cross_entropy_loss(X_train,y_train, l2_r)\n",
    "    print('l2_reg= {:.4f} >> Loss/accuracy d\\'entraînement : {:.3f}'.format(l2_r,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification: Vous devez pouvoir faire du surapprentissage sur quelques échantillons.\n",
    "# Si l'accuracy reste faible, votre implémentation a un bogue.\n",
    "n_check = 5\n",
    "X_check = X_train[:n_check]\n",
    "y_check = y_train[:n_check]\n",
    "model = TwoLayerClassifier( X_check, y_check, X_val, y_val,\n",
    "    num_features=2, num_hidden_neurons=num_hidden_neurons, num_classes=num_classes\n",
    ")\n",
    "\n",
    "loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(num_epochs=200, lr=0.01, l2_reg=0.0)\n",
    "print('Accuracy d\\'entraînement, devrait être 1.0: {:.3f}'.format(accu_train_curve[-1]))\n",
    "if accu_train_curve[-1] < 0.98:\n",
    "    print('ATTENTION: L\\'accuracy n\\'est pas 100%.')\n",
    "    utils.plot_curves(loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve)\n",
    "else:\n",
    "    print('SUCCÈS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier que le fait d'entraîner avec une régularisation L2 croissante augmente la loss et, éventuellement, diminue l'accuracy\n",
    "for l2_r in np.arange(0,1,0.1):\n",
    "    loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(num_epochs=200, lr=0.01, l2_reg=l2_r)\n",
    "    print('l2_reg= {:.4f} >> Loss/accuracy d\\'entraînement : {:.3f} {:.3f}'.format(l2_r,loss_train_curve[-1],accu_train_curve[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie notre modèle; cette fois-ci avec les données complètes.\n",
    "num_hidden_neurons = 20\n",
    "model = TwoLayerClassifier(X_train, y_train, X_val, y_val, num_features=2, \n",
    "                           num_hidden_neurons=num_hidden_neurons, num_classes=num_classes, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = model.train(num_epochs=200, lr=1e-2, l2_reg=0.0,\n",
    "                                                                                 momentum=0.5)\n",
    "\n",
    "# Illustration de la loss et de l'accuracy (le % de biens classés) à chaque itération     \n",
    "utils.plot_curves(loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve)\n",
    "\n",
    "print('[Training]   Loss: {:.3f}   Accuracy: {:.3f}'.format(loss_train_curve[-1], accu_train_curve[-1]))\n",
    "print('[Validation] Loss: {:.3f}   Accuracy: {:.3f}'.format(loss_val_curve[-1], accu_val_curve[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the best hyperparameters lr and l2_reg\n",
    "lr_choices = [1e-4, 1e-3, 1e-2]\n",
    "reg_choices = [1e-1, 1e-2, 1e-3, 1e-4, 0]\n",
    "lr_decay = 1.0  # 0.995  # learning rate is multiplied by this factor after each step\n",
    "\n",
    "best_accu = -1\n",
    "best_params = None\n",
    "best_model = None\n",
    "best_curves = None\n",
    "\n",
    "for lr, reg in itertools.product(lr_choices, reg_choices):\n",
    "    params = (lr, reg)\n",
    "    curves = model.train(num_epochs=50, lr=lr, l2_reg=reg, lr_decay=lr_decay,momentum=0.5)\n",
    "    loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = curves\n",
    "    \n",
    "    val_accu = accu_val_curve[-1]\n",
    "    if val_accu > best_accu:\n",
    "        print('Best val accuracy: {:.3f} | lr: {:.0e} | l2_reg: {:.0e}'.format(val_accu, lr, reg))\n",
    "        best_accu = val_accu\n",
    "        best_params = params\n",
    "        best_model = model\n",
    "        best_curves = curves\n",
    "    else:\n",
    "        print('accuracy: {:.3f} | lr: {:.0e} | l2_reg: {:.0e}'.format(val_accu, lr, reg))\n",
    "        \n",
    "model = best_model\n",
    "utils.plot_curves(*best_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On ré-entraîne le modèle avec les meilleurs hyper-paramètres\n",
    "lr, reg = best_params\n",
    "print(best_params)\n",
    "curves = model.train(num_epochs=200, lr=lr, l2_reg=reg,momentum=0.5)\n",
    "loss_train_curve, loss_val_curve, accu_train_curve, accu_val_curve = curves\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "accu = (pred == y_test).mean()\n",
    "print('Test accuracy: {:.3f}'.format(accu))\n",
    "utils.plot_curves(*curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracer les frontières de décision\n",
    "\n",
    "Nous allons créer une grille de points 2D qui recouvre les données, et nous allons prédire la classe pour chacun de ces points dans l'espace. Cela nous permettra de visualiser les frontières de décision apprises. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats\n",
    "\n",
    "h = 0.05  # contrôle la résolution de la grille\n",
    "x_min, x_max = X_[:, 0].min() - .5, X_[:, 0].max() + .5  # Limites de la grille\n",
    "y_min, y_max = X_[:, 1].min() - .5, X_[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))  # Créer la grille\n",
    "\n",
    "X_predict = np.c_[xx.ravel(), yy.ravel()]  # Convertir la grille en une liste de points\n",
    "Z = model.predict(X_predict)  # Classifier chaque point de la grille\n",
    "Z = Z.reshape(xx.shape)  # Remettre en 2D\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)  # Colorier les cases selon les prédictions\n",
    "\n",
    "X_plot, y_plot = X_, y_\n",
    "plt.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolors='k', cmap=plt.cm.Paired)  # Tracer les données\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.title('Frontières de décision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
